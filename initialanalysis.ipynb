{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel(\"./data/FINAL_DATASET.xlsx\")\n",
    "\n",
    "for name in df.columns:\n",
    "    newname = name\n",
    "    # remove alla characted within two []\n",
    "    while \"[\" in newname:\n",
    "        start = newname.find(\"[\")\n",
    "        end = newname.find(\"]\")\n",
    "        newname = newname[:start] + newname[end+1:]\n",
    "    # remove end spaces and beginning spaces\n",
    "    newname = newname.strip()\n",
    "    # remove alla mathematical oipreators caharacters and spaces\n",
    "    newname = newname.replace(\" \", \"_\")\n",
    "    newname = newname.replace(\">\", \"gt\")\n",
    "    newname = newname.replace(\"<\", \"lt\")\n",
    "    newname = newname.replace(\"=\", \"eq\")\n",
    "    newname = newname.replace(\"+\", \"plus\")\n",
    "    newname = newname.replace(\"-\", \"minus\")\n",
    "    newname = newname.replace(\"*\", \"times\")\n",
    "    newname = newname.replace(\"/\", \"div\")\n",
    "    # rename columns in the dataframe\n",
    "    df.rename(columns={name: newname}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelname  = \"d33\"\n",
    "materialfeatures = ['octahedra_volume_min', 'octahedra_volume_max', 'octahedra_volume_avg',\n",
    "       'octahedra_meanangle_axis_min', 'octahedra_meanangle_axis_max',\n",
    "       'octahedra_meanangle_axis_avg', 'tilt_BOB_ip_min', 'tilt_BOB_ip_max',\n",
    "       'tilt_BOB_ip_avg', 'tilt_BOB_oop_min', 'tilt_BOB_oop_max',\n",
    "       'tilt_BOB_oop_avg', 'spageGroup_no', 'lattice_a', 'lattice_b',\n",
    "       'lattice_c', 'lattice_alfa', 'lattice_beta', 'lattice_gamma',\n",
    "       'volume_uc', 'volume_uc_per_atom', 'volume_ratio_ucVSoctahedra',\n",
    "       'tolerance_factor', 'ratio_outVSinplaneAVG', 'ratio_outVSinplanemin',\n",
    "       'ratio_outVSinplanemax', 'bond_lengthAA_min', 'bond_lengthAA_max',\n",
    "       'bond_lengthAA_avg', 'bond_lengthAB_min', 'bond_lengthAB_max',\n",
    "       'bond_lengthAB_avg', 'bond_lengthAO_min', 'bond_lengthAO_max',\n",
    "       'bond_lengthAO_avg', 'bond_lengthBO_min', 'bond_lengthBO_max',\n",
    "       'bond_lengthBO_avg', 'bond_lengthBB_min', 'bond_lengthBB_max',\n",
    "       'bond_lengthBB_avg', 'is_magnetic', 'is_metallic', 'is_perovskite', \n",
    "       'P_0_z', 'P_0_z_muCdivcm2',]\n",
    "atomicfeatures = ['A_Z', 'A_group', 'A_valence',\n",
    "       'A_vecdivZ', 'A_n_d', 'A_atomic_volume_pymatgen', 'A_Rdce', 'A_Rdve',\n",
    "       'A_rs_max', 'A_rd_max', 'A_IE_ionization_energy',\n",
    "       'A_EA_electron_affinity', 'A_Mulliken', 'A_Pauling',\n",
    "       'A_MartynovminusBatsanov', 'A_atomic_radius_rahm', 'A_vdw_radius_uff',\n",
    "       'A_ionic_radius', 'B_Z', 'B_group', 'B_valence', 'B_vecdivZ', 'B_n_d',\n",
    "       'B_atomic_volume_pymatgen', 'B_Rdce', 'B_Rdve', 'B_rs_max', 'B_rd_max',\n",
    "       'B_IE_ionization_energy', 'B_EA_electron_affinity', 'B_Mulliken',\n",
    "       'B_Pauling', 'B_MartynovminusBatsanov', 'B_atomic_radius_rahm',\n",
    "       'B_vdw_radius_uff', 'B_residual_d', 'B_ionic_radius']\n",
    "\n",
    "Y = df[labelname]\n",
    "X = df[materialfeatures + atomicfeatures]\n",
    "X_mat = df[materialfeatures]\n",
    "X_atm = df[atomicfeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRCUT = 0.95\n",
    "# compute and plot correlation matrix\n",
    "corr = X.corr()\n",
    "# remove highly correlated values\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >CORRCUT)]\n",
    "print(\"Dropping features: \", to_drop)\n",
    "# Drop features\n",
    "X = X.drop(X[to_drop], axis=1)\n",
    "\n",
    "corr_mat = X_mat.corr()\n",
    "# remove highly correlated values\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))\n",
    "print(\"Dropping features: \", to_drop)\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > CORRCUT)]\n",
    "# Drop features\n",
    "X_mat = X_mat.drop(X_mat[to_drop], axis=1)\n",
    "\n",
    "corr_atm = X_atm.corr()\n",
    "# remove highly correlated values\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_atm.where(np.triu(np.ones(corr_atm.shape), k=1).astype(np.bool))\n",
    "print(\"Dropping features: \", to_drop)\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > CORRCUT)]\n",
    "# Drop features\n",
    "X_atm = X_atm.drop(X_atm[to_drop], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_mat = StandardScaler().fit_transform(X_mat)\n",
    "X_atm = StandardScaler().fit_transform(X_atm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sumofvar = []\n",
    "ratioofvar = []\n",
    "\n",
    "first80 = False\n",
    "firtt90 = False\n",
    "for nc in range(1, X.shape[1]):\n",
    "    pca = PCA(n_components=nc)\n",
    "    pcs = pca.fit_transform(X)\n",
    "    sumofvar.append(sum(pca.explained_variance_ratio_))\n",
    "    #print(\"Explained variance ratio for \", nc, \" components: \", pca.explained_variance_ratio_)\n",
    "    ratioofvar.append(pca.explained_variance_ratio_[-1])\n",
    "    if sumofvar[-1] > 0.8 and not first80:\n",
    "        print(\"Number of components for 80% explained variance: \", nc, \" of \", X.shape[1])\n",
    "        first80 = True\n",
    "    if sumofvar[-1] > 0.9 and not firtt90:\n",
    "        print(\"Number of components for 90% explained variance: \", nc, \" of \", X.shape[1])\n",
    "        firtt90 = True\n",
    "    #print(\"Explained variance ratio for \", nc, \" components: \", pca.explained_variance_ratio_)\n",
    "    #print(\"Explained variance for \", nc, \" components: \", pca.explained_variance_)\n",
    "    #print(\"Sum of explained variance for \", nc, \" components: \", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# plot the explained variance ratio\n",
    "plt.plot(sumofvar)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Explained variance\")\n",
    "plt.title(\"Explained variance ratio\")\n",
    "plt.show()\n",
    "\n",
    "# plot the explained variance ratio\n",
    "plt.plot(ratioofvar)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Explained variance\")\n",
    "plt.title(\"Explained variance ratio\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
